# local_developer_agent
The simplest way to get a local developer that can convert your requirements into files, envs, code, and outputs.

# Prerequisites
Running Docker instance
Download Ollama (this example uses llama3.2-vision - just replace with whatever ollama you have running locally
'''
pip install crewai
pip install litellm
'''
